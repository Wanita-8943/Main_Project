{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wanita-8943/Main_Project/blob/main/M4_Train_Unfreez_500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKSs7cyoPHcD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LoeZxmVPMxp",
        "outputId": "f41d2889-555a-4556-eb05-e204fc863ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "1pX9g1HxPM2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "width = 150\n",
        "height = 150\n",
        "epochs = 500\n",
        "NUM_TRAIN = 1425\n",
        "NUM_TEST = 475\n",
        "dropout_rate = 0.2\n",
        "input_shape = (height, width, 3)"
      ],
      "metadata": {
        "id": "eSFtvGyvPM6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb4K4CsMPNAW",
        "outputId": "0bc52724-f56e-4547-f64e-f4c96a952601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Total 837 (delta 0), reused 0 (delta 0), pack-reused 837\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.85 MiB | 5.88 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "eyBg0dLKPND3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoxI-q8-1giK",
        "outputId": "a59b1497-b3d4-49f2-d096-26ab8ec637c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od4Lp6dD1lWK",
        "outputId": "a3d31c5f-8e2d-49cc-a791-1bd48cb3186d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/TVT_Male125'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Jwpq_-KvPef8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load model"
      ],
      "metadata": {
        "id": "od-ZSNm5PoGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/cut_panoramic/Model/Classification/Male/33_รอบที่3_Flimpano_Male125_250.h5')\n",
        "\n",
        "from efficientnet.layers import Swish, DropConnect\n",
        "from efficientnet.model import ConvKernalInitializer\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'ConvKernalInitializer': ConvKernalInitializer,\n",
        "    'Swish': Swish,\n",
        "    'DropConnect':DropConnect\n",
        "})"
      ],
      "metadata": {
        "id": "n5iPL5MNPkhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model \n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/cut_panoramic/Model/Classification/Male/33_รอบที่3_Flimpano_Male125_250.h5')\n",
        "height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "plYz49xMPkly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6IOPBflFbvc",
        "outputId": "d6e3d00b-34fb-4381-af44-c1e2589da0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 19)                24339     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,073,903\n",
            "Trainable params: 24,339\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, #โมเดลส่วนใหญ่ต้องใช้ RGB ในช่วง 0–1\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBMZbdr2Pgw4",
        "outputId": "aa9d5b8b-6355-4fdb-f74a-42df62eefd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1425 images belonging to 19 classes.\n",
            "Found 475 images belonging to 19 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply_16\n",
        "# set 'multiply_16' and following layers trainable\n",
        "model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'multiply_16':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False  "
      ],
      "metadata": {
        "id": "2SmWJJlZ0K0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqOZnbxjGmFY",
        "outputId": "775d3f74-bb50-4b4f-978b-d03c5639e056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 19)                24339     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,073,903\n",
            "Trainable params: 4,031,887\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9Cf1dwyP1PD",
        "outputId": "46e9c532-cd7a-4046-e62c-7140f061cef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-caa7b37242a8>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "89/89 [==============================] - 174s 2s/step - loss: 2.1828 - acc: 0.2761 - val_loss: 2.6911 - val_acc: 0.1659\n",
            "Epoch 2/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1888 - acc: 0.2676 - val_loss: 2.6822 - val_acc: 0.1616\n",
            "Epoch 3/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 2.2239 - acc: 0.2441 - val_loss: 2.6694 - val_acc: 0.1659\n",
            "Epoch 4/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.2530 - acc: 0.2427 - val_loss: 2.6708 - val_acc: 0.1616\n",
            "Epoch 5/500\n",
            "89/89 [==============================] - 23s 250ms/step - loss: 2.1632 - acc: 0.2640 - val_loss: 2.7113 - val_acc: 0.1638\n",
            "Epoch 6/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.1830 - acc: 0.2590 - val_loss: 2.7157 - val_acc: 0.1530\n",
            "Epoch 7/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.2235 - acc: 0.2300 - val_loss: 2.7193 - val_acc: 0.1552\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - 28s 315ms/step - loss: 2.2086 - acc: 0.2512 - val_loss: 2.7119 - val_acc: 0.1659\n",
            "Epoch 9/500\n",
            "89/89 [==============================] - 23s 252ms/step - loss: 2.2079 - acc: 0.2456 - val_loss: 2.7004 - val_acc: 0.1616\n",
            "Epoch 10/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.2050 - acc: 0.2598 - val_loss: 2.7103 - val_acc: 0.1659\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.2252 - acc: 0.2399 - val_loss: 2.6797 - val_acc: 0.1681\n",
            "Epoch 12/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 2.1971 - acc: 0.2647 - val_loss: 2.6793 - val_acc: 0.1530\n",
            "Epoch 13/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.2004 - acc: 0.2583 - val_loss: 2.7030 - val_acc: 0.1659\n",
            "Epoch 14/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.2020 - acc: 0.2661 - val_loss: 2.7154 - val_acc: 0.1573\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.2007 - acc: 0.2456 - val_loss: 2.6949 - val_acc: 0.1659\n",
            "Epoch 16/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1879 - acc: 0.2484 - val_loss: 2.7071 - val_acc: 0.1703\n",
            "Epoch 17/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 2.1856 - acc: 0.2640 - val_loss: 2.7017 - val_acc: 0.1595\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.1893 - acc: 0.2754 - val_loss: 2.7009 - val_acc: 0.1595\n",
            "Epoch 19/500\n",
            "89/89 [==============================] - 21s 229ms/step - loss: 2.2076 - acc: 0.2520 - val_loss: 2.6820 - val_acc: 0.1638\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - 27s 287ms/step - loss: 2.2327 - acc: 0.2378 - val_loss: 2.7229 - val_acc: 0.1638\n",
            "Epoch 21/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.2426 - acc: 0.2363 - val_loss: 2.6820 - val_acc: 0.1573\n",
            "Epoch 22/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1918 - acc: 0.2583 - val_loss: 2.7048 - val_acc: 0.1638\n",
            "Epoch 23/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.2624 - acc: 0.2463 - val_loss: 2.6798 - val_acc: 0.1681\n",
            "Epoch 24/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.2121 - acc: 0.2534 - val_loss: 2.7045 - val_acc: 0.1681\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - 22s 246ms/step - loss: 2.2021 - acc: 0.2484 - val_loss: 2.6901 - val_acc: 0.1552\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 2.1949 - acc: 0.2520 - val_loss: 2.6978 - val_acc: 0.1595\n",
            "Epoch 27/500\n",
            "89/89 [==============================] - 20s 223ms/step - loss: 2.2179 - acc: 0.2612 - val_loss: 2.7242 - val_acc: 0.1573\n",
            "Epoch 28/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.2351 - acc: 0.2520 - val_loss: 2.7058 - val_acc: 0.1638\n",
            "Epoch 29/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.2246 - acc: 0.2640 - val_loss: 2.7107 - val_acc: 0.1638\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.2039 - acc: 0.2605 - val_loss: 2.6801 - val_acc: 0.1659\n",
            "Epoch 31/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1400 - acc: 0.2768 - val_loss: 2.6861 - val_acc: 0.1616\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 2.1567 - acc: 0.2903 - val_loss: 2.6980 - val_acc: 0.1509\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 2.1769 - acc: 0.2534 - val_loss: 2.7001 - val_acc: 0.1616\n",
            "Epoch 34/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 2.1884 - acc: 0.2569 - val_loss: 2.7072 - val_acc: 0.1509\n",
            "Epoch 35/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.2733 - acc: 0.2484 - val_loss: 2.7144 - val_acc: 0.1595\n",
            "Epoch 36/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 2.2323 - acc: 0.2314 - val_loss: 2.7045 - val_acc: 0.1616\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.1797 - acc: 0.2576 - val_loss: 2.7025 - val_acc: 0.1659\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 2.1865 - acc: 0.2669 - val_loss: 2.6842 - val_acc: 0.1681\n",
            "Epoch 39/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.1911 - acc: 0.2576 - val_loss: 2.6906 - val_acc: 0.1595\n",
            "Epoch 40/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1540 - acc: 0.2669 - val_loss: 2.6815 - val_acc: 0.1703\n",
            "Epoch 41/500\n",
            "89/89 [==============================] - 21s 223ms/step - loss: 2.2135 - acc: 0.2583 - val_loss: 2.7038 - val_acc: 0.1659\n",
            "Epoch 42/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1997 - acc: 0.2520 - val_loss: 2.6976 - val_acc: 0.1659\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - 22s 238ms/step - loss: 2.1920 - acc: 0.2633 - val_loss: 2.7101 - val_acc: 0.1595\n",
            "Epoch 44/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1435 - acc: 0.2718 - val_loss: 2.7038 - val_acc: 0.1573\n",
            "Epoch 45/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 2.1826 - acc: 0.2697 - val_loss: 2.6987 - val_acc: 0.1595\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 2.1829 - acc: 0.2661 - val_loss: 2.6755 - val_acc: 0.1638\n",
            "Epoch 47/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.2583 - acc: 0.2520 - val_loss: 2.7122 - val_acc: 0.1638\n",
            "Epoch 48/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.2187 - acc: 0.2626 - val_loss: 2.7238 - val_acc: 0.1638\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.1970 - acc: 0.2690 - val_loss: 2.6954 - val_acc: 0.1681\n",
            "Epoch 50/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.2095 - acc: 0.2576 - val_loss: 2.6909 - val_acc: 0.1530\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.2300 - acc: 0.2434 - val_loss: 2.7051 - val_acc: 0.1595\n",
            "Epoch 52/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.2024 - acc: 0.2697 - val_loss: 2.6921 - val_acc: 0.1552\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - 22s 244ms/step - loss: 2.1605 - acc: 0.2640 - val_loss: 2.7220 - val_acc: 0.1573\n",
            "Epoch 54/500\n",
            "89/89 [==============================] - 21s 223ms/step - loss: 2.1997 - acc: 0.2796 - val_loss: 2.7253 - val_acc: 0.1552\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.2367 - acc: 0.2420 - val_loss: 2.7020 - val_acc: 0.1616\n",
            "Epoch 56/500\n",
            "89/89 [==============================] - 25s 280ms/step - loss: 2.1723 - acc: 0.2697 - val_loss: 2.6837 - val_acc: 0.1530\n",
            "Epoch 57/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 2.2153 - acc: 0.2562 - val_loss: 2.7060 - val_acc: 0.1638\n",
            "Epoch 58/500\n",
            "89/89 [==============================] - 22s 235ms/step - loss: 2.2186 - acc: 0.2484 - val_loss: 2.6908 - val_acc: 0.1552\n",
            "Epoch 59/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.2274 - acc: 0.2527 - val_loss: 2.7108 - val_acc: 0.1638\n",
            "Epoch 60/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 2.1792 - acc: 0.2420 - val_loss: 2.6879 - val_acc: 0.1573\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 2.2196 - acc: 0.2406 - val_loss: 2.7257 - val_acc: 0.1552\n",
            "Epoch 62/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1577 - acc: 0.2711 - val_loss: 2.6906 - val_acc: 0.1616\n",
            "Epoch 63/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1791 - acc: 0.2619 - val_loss: 2.7206 - val_acc: 0.1530\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.2121 - acc: 0.2590 - val_loss: 2.7241 - val_acc: 0.1552\n",
            "Epoch 65/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1517 - acc: 0.2520 - val_loss: 2.7007 - val_acc: 0.1638\n",
            "Epoch 66/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1508 - acc: 0.2761 - val_loss: 2.7011 - val_acc: 0.1659\n",
            "Epoch 67/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.1828 - acc: 0.2598 - val_loss: 2.7075 - val_acc: 0.1638\n",
            "Epoch 68/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.2452 - acc: 0.2420 - val_loss: 2.7166 - val_acc: 0.1552\n",
            "Epoch 69/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.2015 - acc: 0.2612 - val_loss: 2.6876 - val_acc: 0.1530\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1902 - acc: 0.2562 - val_loss: 2.6998 - val_acc: 0.1552\n",
            "Epoch 71/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1920 - acc: 0.2449 - val_loss: 2.6872 - val_acc: 0.1616\n",
            "Epoch 72/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1592 - acc: 0.2768 - val_loss: 2.6849 - val_acc: 0.1681\n",
            "Epoch 73/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.1904 - acc: 0.2541 - val_loss: 2.7192 - val_acc: 0.1595\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 2.1918 - acc: 0.2520 - val_loss: 2.7055 - val_acc: 0.1659\n",
            "Epoch 75/500\n",
            "89/89 [==============================] - 22s 244ms/step - loss: 2.2064 - acc: 0.2541 - val_loss: 2.7122 - val_acc: 0.1595\n",
            "Epoch 76/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.2008 - acc: 0.2619 - val_loss: 2.7293 - val_acc: 0.1595\n",
            "Epoch 77/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1770 - acc: 0.2725 - val_loss: 2.7130 - val_acc: 0.1573\n",
            "Epoch 78/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 2.1844 - acc: 0.2683 - val_loss: 2.6831 - val_acc: 0.1595\n",
            "Epoch 79/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1595 - acc: 0.2612 - val_loss: 2.7052 - val_acc: 0.1573\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 2.2551 - acc: 0.2576 - val_loss: 2.7285 - val_acc: 0.1509\n",
            "Epoch 81/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.1871 - acc: 0.2626 - val_loss: 2.7137 - val_acc: 0.1595\n",
            "Epoch 82/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1438 - acc: 0.2732 - val_loss: 2.6936 - val_acc: 0.1509\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1513 - acc: 0.2732 - val_loss: 2.7210 - val_acc: 0.1724\n",
            "Epoch 84/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.2000 - acc: 0.2477 - val_loss: 2.7186 - val_acc: 0.1573\n",
            "Epoch 85/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 2.1528 - acc: 0.2633 - val_loss: 2.7227 - val_acc: 0.1509\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.1915 - acc: 0.2654 - val_loss: 2.7188 - val_acc: 0.1530\n",
            "Epoch 87/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 2.1883 - acc: 0.2605 - val_loss: 2.7180 - val_acc: 0.1530\n",
            "Epoch 88/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.2030 - acc: 0.2583 - val_loss: 2.7131 - val_acc: 0.1573\n",
            "Epoch 89/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1508 - acc: 0.2654 - val_loss: 2.7240 - val_acc: 0.1552\n",
            "Epoch 90/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1933 - acc: 0.2527 - val_loss: 2.6889 - val_acc: 0.1681\n",
            "Epoch 91/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.2266 - acc: 0.2590 - val_loss: 2.6996 - val_acc: 0.1595\n",
            "Epoch 92/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1998 - acc: 0.2555 - val_loss: 2.7009 - val_acc: 0.1681\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.2357 - acc: 0.2427 - val_loss: 2.7061 - val_acc: 0.1659\n",
            "Epoch 94/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1487 - acc: 0.2789 - val_loss: 2.6846 - val_acc: 0.1638\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1563 - acc: 0.2711 - val_loss: 2.7090 - val_acc: 0.1573\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1459 - acc: 0.2796 - val_loss: 2.7022 - val_acc: 0.1616\n",
            "Epoch 97/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 2.1692 - acc: 0.2697 - val_loss: 2.7095 - val_acc: 0.1616\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1324 - acc: 0.2740 - val_loss: 2.7204 - val_acc: 0.1616\n",
            "Epoch 99/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.2095 - acc: 0.2307 - val_loss: 2.7124 - val_acc: 0.1595\n",
            "Epoch 100/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1337 - acc: 0.2711 - val_loss: 2.7279 - val_acc: 0.1552\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1983 - acc: 0.2661 - val_loss: 2.7372 - val_acc: 0.1573\n",
            "Epoch 102/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1801 - acc: 0.2846 - val_loss: 2.7173 - val_acc: 0.1595\n",
            "Epoch 103/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 2.1621 - acc: 0.2654 - val_loss: 2.7210 - val_acc: 0.1530\n",
            "Epoch 104/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1453 - acc: 0.2747 - val_loss: 2.7236 - val_acc: 0.1466\n",
            "Epoch 105/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 2.1624 - acc: 0.2740 - val_loss: 2.7246 - val_acc: 0.1509\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.2006 - acc: 0.2669 - val_loss: 2.7241 - val_acc: 0.1530\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1868 - acc: 0.2697 - val_loss: 2.7431 - val_acc: 0.1552\n",
            "Epoch 108/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 2.1587 - acc: 0.2811 - val_loss: 2.7116 - val_acc: 0.1616\n",
            "Epoch 109/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1377 - acc: 0.2803 - val_loss: 2.7359 - val_acc: 0.1659\n",
            "Epoch 110/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 2.1732 - acc: 0.2661 - val_loss: 2.7138 - val_acc: 0.1616\n",
            "Epoch 111/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1581 - acc: 0.2647 - val_loss: 2.7352 - val_acc: 0.1703\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1793 - acc: 0.2590 - val_loss: 2.7194 - val_acc: 0.1616\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1660 - acc: 0.2754 - val_loss: 2.7197 - val_acc: 0.1595\n",
            "Epoch 114/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1425 - acc: 0.2626 - val_loss: 2.7204 - val_acc: 0.1638\n",
            "Epoch 115/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1939 - acc: 0.2676 - val_loss: 2.7322 - val_acc: 0.1659\n",
            "Epoch 116/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.1789 - acc: 0.2548 - val_loss: 2.7124 - val_acc: 0.1595\n",
            "Epoch 117/500\n",
            "89/89 [==============================] - 21s 224ms/step - loss: 2.1960 - acc: 0.2484 - val_loss: 2.7287 - val_acc: 0.1573\n",
            "Epoch 118/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 2.1462 - acc: 0.2825 - val_loss: 2.7042 - val_acc: 0.1616\n",
            "Epoch 119/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1805 - acc: 0.2669 - val_loss: 2.7208 - val_acc: 0.1530\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 2.1741 - acc: 0.2676 - val_loss: 2.7329 - val_acc: 0.1573\n",
            "Epoch 121/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1484 - acc: 0.2654 - val_loss: 2.7331 - val_acc: 0.1422\n",
            "Epoch 122/500\n",
            "89/89 [==============================] - 27s 284ms/step - loss: 2.1721 - acc: 0.2761 - val_loss: 2.7382 - val_acc: 0.1487\n",
            "Epoch 123/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1626 - acc: 0.2704 - val_loss: 2.7509 - val_acc: 0.1487\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 2.1582 - acc: 0.2789 - val_loss: 2.7290 - val_acc: 0.1616\n",
            "Epoch 125/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1407 - acc: 0.2740 - val_loss: 2.7137 - val_acc: 0.1509\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 2.1925 - acc: 0.2534 - val_loss: 2.7587 - val_acc: 0.1530\n",
            "Epoch 127/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 2.1664 - acc: 0.2498 - val_loss: 2.7374 - val_acc: 0.1530\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 2.1647 - acc: 0.2811 - val_loss: 2.7449 - val_acc: 0.1573\n",
            "Epoch 129/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1728 - acc: 0.2527 - val_loss: 2.7120 - val_acc: 0.1530\n",
            "Epoch 130/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 2.2111 - acc: 0.2640 - val_loss: 2.7318 - val_acc: 0.1595\n",
            "Epoch 131/500\n",
            "89/89 [==============================] - 25s 279ms/step - loss: 2.1820 - acc: 0.2647 - val_loss: 2.7163 - val_acc: 0.1638\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1507 - acc: 0.2867 - val_loss: 2.7248 - val_acc: 0.1573\n",
            "Epoch 133/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1409 - acc: 0.2690 - val_loss: 2.7408 - val_acc: 0.1530\n",
            "Epoch 134/500\n",
            "89/89 [==============================] - 22s 243ms/step - loss: 2.1509 - acc: 0.2612 - val_loss: 2.7224 - val_acc: 0.1530\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - 27s 287ms/step - loss: 2.1483 - acc: 0.2796 - val_loss: 2.7031 - val_acc: 0.1573\n",
            "Epoch 136/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1617 - acc: 0.2789 - val_loss: 2.7480 - val_acc: 0.1487\n",
            "Epoch 137/500\n",
            "89/89 [==============================] - 21s 225ms/step - loss: 2.1608 - acc: 0.2789 - val_loss: 2.7368 - val_acc: 0.1552\n",
            "Epoch 138/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 2.1725 - acc: 0.2477 - val_loss: 2.7137 - val_acc: 0.1573\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1867 - acc: 0.2683 - val_loss: 2.7332 - val_acc: 0.1595\n",
            "Epoch 140/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1381 - acc: 0.2818 - val_loss: 2.7467 - val_acc: 0.1509\n",
            "Epoch 141/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1714 - acc: 0.2576 - val_loss: 2.7430 - val_acc: 0.1487\n",
            "Epoch 142/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 2.1411 - acc: 0.2796 - val_loss: 2.7154 - val_acc: 0.1552\n",
            "Epoch 143/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.2380 - acc: 0.2534 - val_loss: 2.7303 - val_acc: 0.1573\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - 25s 276ms/step - loss: 2.1322 - acc: 0.2839 - val_loss: 2.7332 - val_acc: 0.1487\n",
            "Epoch 145/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1822 - acc: 0.2661 - val_loss: 2.7255 - val_acc: 0.1552\n",
            "Epoch 146/500\n",
            "89/89 [==============================] - 23s 250ms/step - loss: 2.1403 - acc: 0.2775 - val_loss: 2.7411 - val_acc: 0.1530\n",
            "Epoch 147/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1967 - acc: 0.2498 - val_loss: 2.7565 - val_acc: 0.1616\n",
            "Epoch 148/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1276 - acc: 0.2952 - val_loss: 2.7560 - val_acc: 0.1530\n",
            "Epoch 149/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1944 - acc: 0.2576 - val_loss: 2.7434 - val_acc: 0.1530\n",
            "Epoch 150/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.1553 - acc: 0.2676 - val_loss: 2.7561 - val_acc: 0.1487\n",
            "Epoch 151/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.1677 - acc: 0.2746 - val_loss: 2.7397 - val_acc: 0.1487\n",
            "Epoch 152/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1922 - acc: 0.2583 - val_loss: 2.7453 - val_acc: 0.1530\n",
            "Epoch 153/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1345 - acc: 0.2754 - val_loss: 2.7427 - val_acc: 0.1573\n",
            "Epoch 154/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1530 - acc: 0.2626 - val_loss: 2.7525 - val_acc: 0.1530\n",
            "Epoch 155/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 2.1554 - acc: 0.2697 - val_loss: 2.7421 - val_acc: 0.1573\n",
            "Epoch 156/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 2.0948 - acc: 0.3023 - val_loss: 2.7568 - val_acc: 0.1552\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1579 - acc: 0.2768 - val_loss: 2.7527 - val_acc: 0.1509\n",
            "Epoch 158/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1655 - acc: 0.2832 - val_loss: 2.7491 - val_acc: 0.1509\n",
            "Epoch 159/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 2.1656 - acc: 0.2619 - val_loss: 2.7527 - val_acc: 0.1573\n",
            "Epoch 160/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.1588 - acc: 0.2789 - val_loss: 2.7291 - val_acc: 0.1573\n",
            "Epoch 161/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1483 - acc: 0.2839 - val_loss: 2.7678 - val_acc: 0.1509\n",
            "Epoch 162/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1323 - acc: 0.2732 - val_loss: 2.7567 - val_acc: 0.1444\n",
            "Epoch 163/500\n",
            "89/89 [==============================] - 25s 278ms/step - loss: 2.1788 - acc: 0.2612 - val_loss: 2.7437 - val_acc: 0.1530\n",
            "Epoch 164/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1530 - acc: 0.2818 - val_loss: 2.7145 - val_acc: 0.1595\n",
            "Epoch 165/500\n",
            "89/89 [==============================] - 26s 279ms/step - loss: 2.1243 - acc: 0.2697 - val_loss: 2.7234 - val_acc: 0.1530\n",
            "Epoch 166/500\n",
            "89/89 [==============================] - 21s 226ms/step - loss: 2.1405 - acc: 0.2683 - val_loss: 2.7397 - val_acc: 0.1573\n",
            "Epoch 167/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 2.1929 - acc: 0.2690 - val_loss: 2.7501 - val_acc: 0.1509\n",
            "Epoch 168/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1588 - acc: 0.2782 - val_loss: 2.7432 - val_acc: 0.1616\n",
            "Epoch 169/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1669 - acc: 0.2633 - val_loss: 2.7374 - val_acc: 0.1444\n",
            "Epoch 170/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1644 - acc: 0.2747 - val_loss: 2.7763 - val_acc: 0.1552\n",
            "Epoch 171/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1324 - acc: 0.2903 - val_loss: 2.7603 - val_acc: 0.1509\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.1361 - acc: 0.2761 - val_loss: 2.7297 - val_acc: 0.1466\n",
            "Epoch 173/500\n",
            "89/89 [==============================] - 25s 277ms/step - loss: 2.1238 - acc: 0.2718 - val_loss: 2.7536 - val_acc: 0.1530\n",
            "Epoch 174/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.0967 - acc: 0.2789 - val_loss: 2.7482 - val_acc: 0.1638\n",
            "Epoch 175/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.1586 - acc: 0.2818 - val_loss: 2.7226 - val_acc: 0.1595\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1340 - acc: 0.2605 - val_loss: 2.7178 - val_acc: 0.1616\n",
            "Epoch 177/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1186 - acc: 0.2825 - val_loss: 2.7182 - val_acc: 0.1573\n",
            "Epoch 178/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1444 - acc: 0.2747 - val_loss: 2.7568 - val_acc: 0.1530\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1343 - acc: 0.2768 - val_loss: 2.7366 - val_acc: 0.1573\n",
            "Epoch 180/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1447 - acc: 0.2711 - val_loss: 2.7665 - val_acc: 0.1530\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1629 - acc: 0.2612 - val_loss: 2.7447 - val_acc: 0.1509\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.1342 - acc: 0.2818 - val_loss: 2.7163 - val_acc: 0.1616\n",
            "Epoch 183/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.1678 - acc: 0.2491 - val_loss: 2.7523 - val_acc: 0.1552\n",
            "Epoch 184/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1539 - acc: 0.2676 - val_loss: 2.7450 - val_acc: 0.1530\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1531 - acc: 0.2740 - val_loss: 2.7259 - val_acc: 0.1595\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.1123 - acc: 0.2590 - val_loss: 2.7388 - val_acc: 0.1573\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 2.1501 - acc: 0.2590 - val_loss: 2.7343 - val_acc: 0.1616\n",
            "Epoch 188/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1475 - acc: 0.2598 - val_loss: 2.7368 - val_acc: 0.1638\n",
            "Epoch 189/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1779 - acc: 0.2626 - val_loss: 2.7320 - val_acc: 0.1767\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1930 - acc: 0.2740 - val_loss: 2.7513 - val_acc: 0.1573\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - 22s 239ms/step - loss: 2.1619 - acc: 0.2711 - val_loss: 2.7551 - val_acc: 0.1595\n",
            "Epoch 192/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1190 - acc: 0.2789 - val_loss: 2.7530 - val_acc: 0.1681\n",
            "Epoch 193/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1587 - acc: 0.2818 - val_loss: 2.7399 - val_acc: 0.1616\n",
            "Epoch 194/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1729 - acc: 0.2562 - val_loss: 2.7565 - val_acc: 0.1616\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - 26s 281ms/step - loss: 2.1224 - acc: 0.2576 - val_loss: 2.7596 - val_acc: 0.1530\n",
            "Epoch 196/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1691 - acc: 0.2740 - val_loss: 2.7579 - val_acc: 0.1552\n",
            "Epoch 197/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1317 - acc: 0.2711 - val_loss: 2.7564 - val_acc: 0.1573\n",
            "Epoch 198/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1710 - acc: 0.2725 - val_loss: 2.7365 - val_acc: 0.1530\n",
            "Epoch 199/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.1753 - acc: 0.2612 - val_loss: 2.7337 - val_acc: 0.1638\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.0947 - acc: 0.2881 - val_loss: 2.7268 - val_acc: 0.1638\n",
            "Epoch 201/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1289 - acc: 0.2640 - val_loss: 2.7287 - val_acc: 0.1573\n",
            "Epoch 202/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1341 - acc: 0.2711 - val_loss: 2.7444 - val_acc: 0.1638\n",
            "Epoch 203/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.1617 - acc: 0.2796 - val_loss: 2.7539 - val_acc: 0.1595\n",
            "Epoch 204/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.1519 - acc: 0.2789 - val_loss: 2.7648 - val_acc: 0.1573\n",
            "Epoch 205/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1265 - acc: 0.2711 - val_loss: 2.7557 - val_acc: 0.1616\n",
            "Epoch 206/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1644 - acc: 0.2825 - val_loss: 2.7774 - val_acc: 0.1509\n",
            "Epoch 207/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 2.1210 - acc: 0.2811 - val_loss: 2.7519 - val_acc: 0.1573\n",
            "Epoch 208/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1475 - acc: 0.2718 - val_loss: 2.7574 - val_acc: 0.1638\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1228 - acc: 0.2867 - val_loss: 2.7575 - val_acc: 0.1573\n",
            "Epoch 210/500\n",
            "89/89 [==============================] - 30s 327ms/step - loss: 2.1814 - acc: 0.2725 - val_loss: 2.7681 - val_acc: 0.1595\n",
            "Epoch 211/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.1207 - acc: 0.2853 - val_loss: 2.7668 - val_acc: 0.1573\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - 29s 323ms/step - loss: 2.1151 - acc: 0.2837 - val_loss: 2.7786 - val_acc: 0.1530\n",
            "Epoch 213/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1140 - acc: 0.2782 - val_loss: 2.7838 - val_acc: 0.1530\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1556 - acc: 0.2732 - val_loss: 2.7424 - val_acc: 0.1595\n",
            "Epoch 215/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.1333 - acc: 0.2740 - val_loss: 2.7610 - val_acc: 0.1659\n",
            "Epoch 216/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1390 - acc: 0.2669 - val_loss: 2.7878 - val_acc: 0.1638\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1433 - acc: 0.2803 - val_loss: 2.7640 - val_acc: 0.1595\n",
            "Epoch 218/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1824 - acc: 0.2598 - val_loss: 2.7707 - val_acc: 0.1573\n",
            "Epoch 219/500\n",
            "89/89 [==============================] - 26s 282ms/step - loss: 2.1373 - acc: 0.2718 - val_loss: 2.7782 - val_acc: 0.1573\n",
            "Epoch 220/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 2.1157 - acc: 0.2910 - val_loss: 2.7994 - val_acc: 0.1509\n",
            "Epoch 221/500\n",
            "89/89 [==============================] - 28s 297ms/step - loss: 2.1220 - acc: 0.2846 - val_loss: 2.7713 - val_acc: 0.1595\n",
            "Epoch 222/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.1458 - acc: 0.2775 - val_loss: 2.7728 - val_acc: 0.1638\n",
            "Epoch 223/500\n",
            "89/89 [==============================] - 28s 296ms/step - loss: 2.1924 - acc: 0.2676 - val_loss: 2.7786 - val_acc: 0.1595\n",
            "Epoch 224/500\n",
            "89/89 [==============================] - 28s 298ms/step - loss: 2.1508 - acc: 0.2853 - val_loss: 2.7863 - val_acc: 0.1616\n",
            "Epoch 225/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1179 - acc: 0.2853 - val_loss: 2.7508 - val_acc: 0.1638\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1544 - acc: 0.2874 - val_loss: 2.7462 - val_acc: 0.1638\n",
            "Epoch 227/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.1622 - acc: 0.2768 - val_loss: 2.7613 - val_acc: 0.1573\n",
            "Epoch 228/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.0876 - acc: 0.2867 - val_loss: 2.7641 - val_acc: 0.1638\n",
            "Epoch 229/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 2.1697 - acc: 0.2754 - val_loss: 2.7653 - val_acc: 0.1595\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - 22s 239ms/step - loss: 2.1307 - acc: 0.2647 - val_loss: 2.7719 - val_acc: 0.1638\n",
            "Epoch 231/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1683 - acc: 0.2697 - val_loss: 2.7824 - val_acc: 0.1552\n",
            "Epoch 232/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1386 - acc: 0.2704 - val_loss: 2.7849 - val_acc: 0.1552\n",
            "Epoch 233/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 2.1118 - acc: 0.2818 - val_loss: 2.7651 - val_acc: 0.1573\n",
            "Epoch 234/500\n",
            "89/89 [==============================] - 21s 231ms/step - loss: 2.1607 - acc: 0.2676 - val_loss: 2.7323 - val_acc: 0.1659\n",
            "Epoch 235/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1455 - acc: 0.2718 - val_loss: 2.7421 - val_acc: 0.1509\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.1407 - acc: 0.2661 - val_loss: 2.7359 - val_acc: 0.1659\n",
            "Epoch 237/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.1354 - acc: 0.2718 - val_loss: 2.7581 - val_acc: 0.1616\n",
            "Epoch 238/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1327 - acc: 0.2697 - val_loss: 2.7677 - val_acc: 0.1616\n",
            "Epoch 239/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 2.1429 - acc: 0.2782 - val_loss: 2.7966 - val_acc: 0.1573\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 2.1384 - acc: 0.2803 - val_loss: 2.7825 - val_acc: 0.1638\n",
            "Epoch 241/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.1366 - acc: 0.2960 - val_loss: 2.7577 - val_acc: 0.1595\n",
            "Epoch 242/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1282 - acc: 0.2775 - val_loss: 2.7423 - val_acc: 0.1616\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1013 - acc: 0.2732 - val_loss: 2.7624 - val_acc: 0.1530\n",
            "Epoch 244/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 2.1159 - acc: 0.2938 - val_loss: 2.7508 - val_acc: 0.1681\n",
            "Epoch 245/500\n",
            "89/89 [==============================] - 26s 280ms/step - loss: 2.1239 - acc: 0.2860 - val_loss: 2.7620 - val_acc: 0.1573\n",
            "Epoch 246/500\n",
            "89/89 [==============================] - 27s 289ms/step - loss: 2.1353 - acc: 0.2896 - val_loss: 2.7625 - val_acc: 0.1595\n",
            "Epoch 247/500\n",
            "89/89 [==============================] - 26s 292ms/step - loss: 2.1149 - acc: 0.2732 - val_loss: 2.7773 - val_acc: 0.1659\n",
            "Epoch 248/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.0767 - acc: 0.2903 - val_loss: 2.7494 - val_acc: 0.1595\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1525 - acc: 0.2754 - val_loss: 2.7721 - val_acc: 0.1573\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1586 - acc: 0.2697 - val_loss: 2.7509 - val_acc: 0.1616\n",
            "Epoch 251/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.1552 - acc: 0.2874 - val_loss: 2.7396 - val_acc: 0.1595\n",
            "Epoch 252/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1737 - acc: 0.2605 - val_loss: 2.7604 - val_acc: 0.1595\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - 23s 251ms/step - loss: 2.1238 - acc: 0.2782 - val_loss: 2.7673 - val_acc: 0.1552\n",
            "Epoch 254/500\n",
            "89/89 [==============================] - 22s 237ms/step - loss: 2.1561 - acc: 0.2683 - val_loss: 2.7740 - val_acc: 0.1595\n",
            "Epoch 255/500\n",
            "89/89 [==============================] - 26s 286ms/step - loss: 2.1422 - acc: 0.2803 - val_loss: 2.7572 - val_acc: 0.1595\n",
            "Epoch 256/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 2.1105 - acc: 0.2697 - val_loss: 2.7950 - val_acc: 0.1595\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1548 - acc: 0.2626 - val_loss: 2.7725 - val_acc: 0.1595\n",
            "Epoch 258/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1161 - acc: 0.2825 - val_loss: 2.7618 - val_acc: 0.1616\n",
            "Epoch 259/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1098 - acc: 0.2910 - val_loss: 2.8201 - val_acc: 0.1509\n",
            "Epoch 260/500\n",
            "89/89 [==============================] - 21s 233ms/step - loss: 2.1316 - acc: 0.2825 - val_loss: 2.7840 - val_acc: 0.1530\n",
            "Epoch 261/500\n",
            "89/89 [==============================] - 23s 244ms/step - loss: 2.1262 - acc: 0.2768 - val_loss: 2.7602 - val_acc: 0.1573\n",
            "Epoch 262/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1480 - acc: 0.2725 - val_loss: 2.7831 - val_acc: 0.1552\n",
            "Epoch 263/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.1174 - acc: 0.2832 - val_loss: 2.7943 - val_acc: 0.1573\n",
            "Epoch 264/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1302 - acc: 0.2704 - val_loss: 2.7548 - val_acc: 0.1573\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.1165 - acc: 0.2747 - val_loss: 2.7739 - val_acc: 0.1638\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.1340 - acc: 0.2683 - val_loss: 2.7762 - val_acc: 0.1530\n",
            "Epoch 267/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1464 - acc: 0.2711 - val_loss: 2.7907 - val_acc: 0.1487\n",
            "Epoch 268/500\n",
            "89/89 [==============================] - 29s 322ms/step - loss: 2.0719 - acc: 0.3130 - val_loss: 2.8125 - val_acc: 0.1487\n",
            "Epoch 269/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1363 - acc: 0.2917 - val_loss: 2.7579 - val_acc: 0.1638\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.1243 - acc: 0.2995 - val_loss: 2.7696 - val_acc: 0.1659\n",
            "Epoch 271/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1386 - acc: 0.2825 - val_loss: 2.7662 - val_acc: 0.1595\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.1270 - acc: 0.2704 - val_loss: 2.7677 - val_acc: 0.1616\n",
            "Epoch 273/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.1723 - acc: 0.2697 - val_loss: 2.7572 - val_acc: 0.1681\n",
            "Epoch 274/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1099 - acc: 0.2903 - val_loss: 2.7895 - val_acc: 0.1595\n",
            "Epoch 275/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1301 - acc: 0.2740 - val_loss: 2.7711 - val_acc: 0.1616\n",
            "Epoch 276/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.1657 - acc: 0.2640 - val_loss: 2.7699 - val_acc: 0.1530\n",
            "Epoch 277/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.0974 - acc: 0.3009 - val_loss: 2.7849 - val_acc: 0.1552\n",
            "Epoch 278/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1750 - acc: 0.2661 - val_loss: 2.8054 - val_acc: 0.1595\n",
            "Epoch 279/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.1636 - acc: 0.2732 - val_loss: 2.7561 - val_acc: 0.1616\n",
            "Epoch 280/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 2.1015 - acc: 0.2889 - val_loss: 2.7684 - val_acc: 0.1552\n",
            "Epoch 281/500\n",
            "89/89 [==============================] - 21s 232ms/step - loss: 2.1816 - acc: 0.2782 - val_loss: 2.7518 - val_acc: 0.1595\n",
            "Epoch 282/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.1495 - acc: 0.2789 - val_loss: 2.7612 - val_acc: 0.1573\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1135 - acc: 0.2910 - val_loss: 2.7478 - val_acc: 0.1552\n",
            "Epoch 284/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 2.1458 - acc: 0.2690 - val_loss: 2.7798 - val_acc: 0.1616\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 2.1285 - acc: 0.2825 - val_loss: 2.7611 - val_acc: 0.1509\n",
            "Epoch 286/500\n",
            "89/89 [==============================] - 24s 257ms/step - loss: 2.1342 - acc: 0.2867 - val_loss: 2.7657 - val_acc: 0.1616\n",
            "Epoch 287/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 2.1440 - acc: 0.2718 - val_loss: 2.7711 - val_acc: 0.1638\n",
            "Epoch 288/500\n",
            "89/89 [==============================] - 23s 258ms/step - loss: 2.1148 - acc: 0.2683 - val_loss: 2.7874 - val_acc: 0.1659\n",
            "Epoch 289/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1990 - acc: 0.2732 - val_loss: 2.7772 - val_acc: 0.1681\n",
            "Epoch 290/500\n",
            "89/89 [==============================] - 26s 284ms/step - loss: 2.1639 - acc: 0.2853 - val_loss: 2.7481 - val_acc: 0.1638\n",
            "Epoch 291/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1151 - acc: 0.2914 - val_loss: 2.7921 - val_acc: 0.1595\n",
            "Epoch 292/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1704 - acc: 0.2732 - val_loss: 2.7631 - val_acc: 0.1595\n",
            "Epoch 293/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.0912 - acc: 0.2981 - val_loss: 2.7763 - val_acc: 0.1616\n",
            "Epoch 294/500\n",
            "89/89 [==============================] - 21s 230ms/step - loss: 2.1528 - acc: 0.2775 - val_loss: 2.7650 - val_acc: 0.1552\n",
            "Epoch 295/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1189 - acc: 0.2846 - val_loss: 2.7808 - val_acc: 0.1595\n",
            "Epoch 296/500\n",
            "89/89 [==============================] - 23s 248ms/step - loss: 2.0860 - acc: 0.2811 - val_loss: 2.7722 - val_acc: 0.1573\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - 21s 228ms/step - loss: 2.1354 - acc: 0.2754 - val_loss: 2.7812 - val_acc: 0.1530\n",
            "Epoch 298/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1392 - acc: 0.2782 - val_loss: 2.7656 - val_acc: 0.1573\n",
            "Epoch 299/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1595 - acc: 0.2669 - val_loss: 2.7758 - val_acc: 0.1573\n",
            "Epoch 300/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1135 - acc: 0.2832 - val_loss: 2.7580 - val_acc: 0.1552\n",
            "Epoch 301/500\n",
            "89/89 [==============================] - 25s 275ms/step - loss: 2.1396 - acc: 0.2697 - val_loss: 2.7587 - val_acc: 0.1573\n",
            "Epoch 302/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1223 - acc: 0.2839 - val_loss: 2.7676 - val_acc: 0.1552\n",
            "Epoch 303/500\n",
            "89/89 [==============================] - 27s 286ms/step - loss: 2.1497 - acc: 0.2683 - val_loss: 2.7669 - val_acc: 0.1530\n",
            "Epoch 304/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1722 - acc: 0.2654 - val_loss: 2.7500 - val_acc: 0.1595\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1013 - acc: 0.2789 - val_loss: 2.7554 - val_acc: 0.1595\n",
            "Epoch 306/500\n",
            "89/89 [==============================] - 27s 285ms/step - loss: 2.1252 - acc: 0.2740 - val_loss: 2.7743 - val_acc: 0.1530\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.1424 - acc: 0.2768 - val_loss: 2.7778 - val_acc: 0.1552\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.0784 - acc: 0.2818 - val_loss: 2.7506 - val_acc: 0.1573\n",
            "Epoch 309/500\n",
            "89/89 [==============================] - 26s 283ms/step - loss: 2.0963 - acc: 0.2960 - val_loss: 2.7684 - val_acc: 0.1552\n",
            "Epoch 310/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1272 - acc: 0.2683 - val_loss: 2.7708 - val_acc: 0.1530\n",
            "Epoch 311/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1489 - acc: 0.2952 - val_loss: 2.7857 - val_acc: 0.1595\n",
            "Epoch 312/500\n",
            "89/89 [==============================] - 22s 235ms/step - loss: 2.1563 - acc: 0.2782 - val_loss: 2.7696 - val_acc: 0.1595\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1231 - acc: 0.2846 - val_loss: 2.7619 - val_acc: 0.1681\n",
            "Epoch 314/500\n",
            "89/89 [==============================] - 21s 227ms/step - loss: 2.0987 - acc: 0.2889 - val_loss: 2.7757 - val_acc: 0.1616\n",
            "Epoch 315/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 2.1092 - acc: 0.2931 - val_loss: 2.7590 - val_acc: 0.1595\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1226 - acc: 0.2796 - val_loss: 2.7780 - val_acc: 0.1552\n",
            "Epoch 317/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 2.1252 - acc: 0.2754 - val_loss: 2.7639 - val_acc: 0.1595\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - 26s 288ms/step - loss: 2.1011 - acc: 0.2718 - val_loss: 2.7640 - val_acc: 0.1573\n",
            "Epoch 319/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.0828 - acc: 0.2896 - val_loss: 2.7775 - val_acc: 0.1509\n",
            "Epoch 320/500\n",
            "89/89 [==============================] - 26s 291ms/step - loss: 2.1409 - acc: 0.2775 - val_loss: 2.7683 - val_acc: 0.1573\n",
            "Epoch 321/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.0998 - acc: 0.2917 - val_loss: 2.7719 - val_acc: 0.1552\n",
            "Epoch 322/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.1316 - acc: 0.2661 - val_loss: 2.7732 - val_acc: 0.1487\n",
            "Epoch 323/500\n",
            "89/89 [==============================] - 30s 331ms/step - loss: 2.1439 - acc: 0.2661 - val_loss: 2.7679 - val_acc: 0.1573\n",
            "Epoch 324/500\n",
            "89/89 [==============================] - 22s 240ms/step - loss: 2.1070 - acc: 0.2910 - val_loss: 2.7949 - val_acc: 0.1487\n",
            "Epoch 325/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1577 - acc: 0.2683 - val_loss: 2.7771 - val_acc: 0.1595\n",
            "Epoch 326/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1536 - acc: 0.2874 - val_loss: 2.7866 - val_acc: 0.1616\n",
            "Epoch 327/500\n",
            "89/89 [==============================] - 26s 294ms/step - loss: 2.1410 - acc: 0.2555 - val_loss: 2.8069 - val_acc: 0.1487\n",
            "Epoch 328/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1539 - acc: 0.2669 - val_loss: 2.7757 - val_acc: 0.1616\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 2.1151 - acc: 0.2853 - val_loss: 2.7930 - val_acc: 0.1552\n",
            "Epoch 330/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1572 - acc: 0.2583 - val_loss: 2.8020 - val_acc: 0.1466\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - 26s 285ms/step - loss: 2.1083 - acc: 0.3023 - val_loss: 2.7913 - val_acc: 0.1530\n",
            "Epoch 332/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.0786 - acc: 0.3009 - val_loss: 2.8091 - val_acc: 0.1573\n",
            "Epoch 333/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1168 - acc: 0.2960 - val_loss: 2.7831 - val_acc: 0.1552\n",
            "Epoch 334/500\n",
            "89/89 [==============================] - 26s 287ms/step - loss: 2.1309 - acc: 0.2661 - val_loss: 2.7702 - val_acc: 0.1595\n",
            "Epoch 335/500\n",
            "89/89 [==============================] - 23s 247ms/step - loss: 2.0952 - acc: 0.2874 - val_loss: 2.7904 - val_acc: 0.1595\n",
            "Epoch 336/500\n",
            "89/89 [==============================] - 29s 321ms/step - loss: 2.1033 - acc: 0.2754 - val_loss: 2.7839 - val_acc: 0.1595\n",
            "Epoch 337/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 2.1099 - acc: 0.2739 - val_loss: 2.7815 - val_acc: 0.1552\n",
            "Epoch 338/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.0978 - acc: 0.2967 - val_loss: 2.7879 - val_acc: 0.1616\n",
            "Epoch 339/500\n",
            "89/89 [==============================] - 28s 314ms/step - loss: 2.1466 - acc: 0.2633 - val_loss: 2.7963 - val_acc: 0.1595\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 2.1311 - acc: 0.2832 - val_loss: 2.8055 - val_acc: 0.1573\n",
            "Epoch 341/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.0905 - acc: 0.2860 - val_loss: 2.7836 - val_acc: 0.1595\n",
            "Epoch 342/500\n",
            "89/89 [==============================] - 24s 262ms/step - loss: 2.1552 - acc: 0.2803 - val_loss: 2.7907 - val_acc: 0.1509\n",
            "Epoch 343/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1486 - acc: 0.2761 - val_loss: 2.7735 - val_acc: 0.1638\n",
            "Epoch 344/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.1171 - acc: 0.2796 - val_loss: 2.7787 - val_acc: 0.1616\n",
            "Epoch 345/500\n",
            "89/89 [==============================] - 30s 324ms/step - loss: 2.0537 - acc: 0.2867 - val_loss: 2.7951 - val_acc: 0.1638\n",
            "Epoch 346/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1018 - acc: 0.2832 - val_loss: 2.7765 - val_acc: 0.1659\n",
            "Epoch 347/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.0994 - acc: 0.2931 - val_loss: 2.7983 - val_acc: 0.1552\n",
            "Epoch 348/500\n",
            "89/89 [==============================] - 28s 314ms/step - loss: 2.0962 - acc: 0.2768 - val_loss: 2.7858 - val_acc: 0.1595\n",
            "Epoch 349/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.0957 - acc: 0.2981 - val_loss: 2.7887 - val_acc: 0.1552\n",
            "Epoch 350/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1663 - acc: 0.2732 - val_loss: 2.7930 - val_acc: 0.1573\n",
            "Epoch 351/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.1062 - acc: 0.2952 - val_loss: 2.8127 - val_acc: 0.1552\n",
            "Epoch 352/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1152 - acc: 0.2903 - val_loss: 2.7957 - val_acc: 0.1573\n",
            "Epoch 353/500\n",
            "89/89 [==============================] - 23s 251ms/step - loss: 2.1348 - acc: 0.2917 - val_loss: 2.7933 - val_acc: 0.1573\n",
            "Epoch 354/500\n",
            "89/89 [==============================] - 31s 338ms/step - loss: 2.1166 - acc: 0.2732 - val_loss: 2.7841 - val_acc: 0.1616\n",
            "Epoch 355/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 2.1278 - acc: 0.2981 - val_loss: 2.7712 - val_acc: 0.1724\n",
            "Epoch 356/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1225 - acc: 0.2860 - val_loss: 2.7837 - val_acc: 0.1616\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - 29s 313ms/step - loss: 2.1203 - acc: 0.2811 - val_loss: 2.7997 - val_acc: 0.1552\n",
            "Epoch 358/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1054 - acc: 0.2846 - val_loss: 2.7894 - val_acc: 0.1638\n",
            "Epoch 359/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1337 - acc: 0.2874 - val_loss: 2.7745 - val_acc: 0.1552\n",
            "Epoch 360/500\n",
            "89/89 [==============================] - 31s 339ms/step - loss: 2.1228 - acc: 0.2910 - val_loss: 2.7736 - val_acc: 0.1552\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.0905 - acc: 0.2761 - val_loss: 2.7773 - val_acc: 0.1509\n",
            "Epoch 362/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.1073 - acc: 0.2725 - val_loss: 2.7573 - val_acc: 0.1573\n",
            "Epoch 363/500\n",
            "89/89 [==============================] - 22s 237ms/step - loss: 2.1131 - acc: 0.2775 - val_loss: 2.7764 - val_acc: 0.1595\n",
            "Epoch 364/500\n",
            "89/89 [==============================] - 29s 306ms/step - loss: 2.1372 - acc: 0.2782 - val_loss: 2.8012 - val_acc: 0.1595\n",
            "Epoch 365/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 2.0654 - acc: 0.3073 - val_loss: 2.7947 - val_acc: 0.1573\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - 30s 321ms/step - loss: 2.1260 - acc: 0.2718 - val_loss: 2.7882 - val_acc: 0.1573\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - 29s 314ms/step - loss: 2.1358 - acc: 0.2732 - val_loss: 2.7773 - val_acc: 0.1595\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1073 - acc: 0.2910 - val_loss: 2.7736 - val_acc: 0.1616\n",
            "Epoch 369/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 2.0867 - acc: 0.2775 - val_loss: 2.7838 - val_acc: 0.1595\n",
            "Epoch 370/500\n",
            "89/89 [==============================] - 29s 318ms/step - loss: 2.0541 - acc: 0.2995 - val_loss: 2.8150 - val_acc: 0.1466\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1417 - acc: 0.2612 - val_loss: 2.8198 - val_acc: 0.1616\n",
            "Epoch 372/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1263 - acc: 0.2881 - val_loss: 2.8117 - val_acc: 0.1573\n",
            "Epoch 373/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 2.1273 - acc: 0.2839 - val_loss: 2.8232 - val_acc: 0.1595\n",
            "Epoch 374/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1101 - acc: 0.2761 - val_loss: 2.8010 - val_acc: 0.1573\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.1095 - acc: 0.2796 - val_loss: 2.7981 - val_acc: 0.1638\n",
            "Epoch 376/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.0781 - acc: 0.2903 - val_loss: 2.7655 - val_acc: 0.1573\n",
            "Epoch 377/500\n",
            "89/89 [==============================] - 30s 328ms/step - loss: 2.1459 - acc: 0.2775 - val_loss: 2.7980 - val_acc: 0.1573\n",
            "Epoch 378/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 2.0937 - acc: 0.2818 - val_loss: 2.8044 - val_acc: 0.1616\n",
            "Epoch 379/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1215 - acc: 0.2725 - val_loss: 2.8006 - val_acc: 0.1552\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1011 - acc: 0.2881 - val_loss: 2.8080 - val_acc: 0.1552\n",
            "Epoch 381/500\n",
            "89/89 [==============================] - 29s 317ms/step - loss: 2.0896 - acc: 0.2761 - val_loss: 2.8034 - val_acc: 0.1509\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.0970 - acc: 0.2931 - val_loss: 2.8153 - val_acc: 0.1552\n",
            "Epoch 383/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.0980 - acc: 0.2889 - val_loss: 2.8102 - val_acc: 0.1595\n",
            "Epoch 384/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1231 - acc: 0.2747 - val_loss: 2.7989 - val_acc: 0.1552\n",
            "Epoch 385/500\n",
            "89/89 [==============================] - 22s 238ms/step - loss: 2.1006 - acc: 0.2782 - val_loss: 2.7933 - val_acc: 0.1530\n",
            "Epoch 386/500\n",
            "89/89 [==============================] - 29s 310ms/step - loss: 2.1448 - acc: 0.2676 - val_loss: 2.7765 - val_acc: 0.1552\n",
            "Epoch 387/500\n",
            "89/89 [==============================] - 29s 312ms/step - loss: 2.1108 - acc: 0.2874 - val_loss: 2.8212 - val_acc: 0.1487\n",
            "Epoch 388/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.1480 - acc: 0.2619 - val_loss: 2.7903 - val_acc: 0.1487\n",
            "Epoch 389/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.0830 - acc: 0.2867 - val_loss: 2.7832 - val_acc: 0.1530\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.0769 - acc: 0.2761 - val_loss: 2.7977 - val_acc: 0.1509\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1004 - acc: 0.2697 - val_loss: 2.7899 - val_acc: 0.1444\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1744 - acc: 0.2768 - val_loss: 2.7935 - val_acc: 0.1509\n",
            "Epoch 393/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1283 - acc: 0.2754 - val_loss: 2.7825 - val_acc: 0.1444\n",
            "Epoch 394/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 2.0591 - acc: 0.2995 - val_loss: 2.8087 - val_acc: 0.1530\n",
            "Epoch 395/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1048 - acc: 0.2803 - val_loss: 2.8157 - val_acc: 0.1552\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1166 - acc: 0.2661 - val_loss: 2.8331 - val_acc: 0.1509\n",
            "Epoch 397/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1275 - acc: 0.2775 - val_loss: 2.8173 - val_acc: 0.1573\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1034 - acc: 0.2903 - val_loss: 2.8157 - val_acc: 0.1552\n",
            "Epoch 399/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 2.1489 - acc: 0.2796 - val_loss: 2.8093 - val_acc: 0.1573\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 2.1366 - acc: 0.2718 - val_loss: 2.7952 - val_acc: 0.1530\n",
            "Epoch 401/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1034 - acc: 0.2839 - val_loss: 2.8136 - val_acc: 0.1573\n",
            "Epoch 402/500\n",
            "89/89 [==============================] - 28s 306ms/step - loss: 2.1263 - acc: 0.2896 - val_loss: 2.8092 - val_acc: 0.1638\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.0862 - acc: 0.3013 - val_loss: 2.8316 - val_acc: 0.1573\n",
            "Epoch 404/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.1348 - acc: 0.2931 - val_loss: 2.8250 - val_acc: 0.1530\n",
            "Epoch 405/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 2.0893 - acc: 0.3031 - val_loss: 2.8155 - val_acc: 0.1530\n",
            "Epoch 406/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.1122 - acc: 0.2803 - val_loss: 2.8201 - val_acc: 0.1616\n",
            "Epoch 407/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 2.1017 - acc: 0.2931 - val_loss: 2.7931 - val_acc: 0.1552\n",
            "Epoch 408/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.0851 - acc: 0.2952 - val_loss: 2.7910 - val_acc: 0.1616\n",
            "Epoch 409/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1478 - acc: 0.2654 - val_loss: 2.8089 - val_acc: 0.1573\n",
            "Epoch 410/500\n",
            "89/89 [==============================] - 22s 242ms/step - loss: 2.0663 - acc: 0.2867 - val_loss: 2.8286 - val_acc: 0.1638\n",
            "Epoch 411/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 2.0899 - acc: 0.2896 - val_loss: 2.8149 - val_acc: 0.1616\n",
            "Epoch 412/500\n",
            "89/89 [==============================] - 29s 314ms/step - loss: 2.0914 - acc: 0.2889 - val_loss: 2.8163 - val_acc: 0.1616\n",
            "Epoch 413/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.0841 - acc: 0.2874 - val_loss: 2.8198 - val_acc: 0.1509\n",
            "Epoch 414/500\n",
            "89/89 [==============================] - 28s 307ms/step - loss: 2.0835 - acc: 0.3172 - val_loss: 2.8415 - val_acc: 0.1616\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.1026 - acc: 0.2931 - val_loss: 2.8176 - val_acc: 0.1659\n",
            "Epoch 416/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.0907 - acc: 0.2846 - val_loss: 2.8075 - val_acc: 0.1530\n",
            "Epoch 417/500\n",
            "89/89 [==============================] - 31s 340ms/step - loss: 2.0896 - acc: 0.2952 - val_loss: 2.8366 - val_acc: 0.1616\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - 27s 299ms/step - loss: 2.0939 - acc: 0.2917 - val_loss: 2.8058 - val_acc: 0.1573\n",
            "Epoch 419/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 2.1336 - acc: 0.2825 - val_loss: 2.8170 - val_acc: 0.1638\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.0646 - acc: 0.3059 - val_loss: 2.8030 - val_acc: 0.1595\n",
            "Epoch 421/500\n",
            "89/89 [==============================] - 29s 313ms/step - loss: 2.1250 - acc: 0.2811 - val_loss: 2.8123 - val_acc: 0.1552\n",
            "Epoch 422/500\n",
            "89/89 [==============================] - 29s 314ms/step - loss: 2.1185 - acc: 0.2754 - val_loss: 2.8271 - val_acc: 0.1573\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.0718 - acc: 0.2952 - val_loss: 2.8298 - val_acc: 0.1487\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - 29s 317ms/step - loss: 2.1049 - acc: 0.2732 - val_loss: 2.8414 - val_acc: 0.1552\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.0799 - acc: 0.3094 - val_loss: 2.8288 - val_acc: 0.1638\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1010 - acc: 0.2818 - val_loss: 2.8346 - val_acc: 0.1573\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1277 - acc: 0.2825 - val_loss: 2.8519 - val_acc: 0.1487\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.0881 - acc: 0.2981 - val_loss: 2.8407 - val_acc: 0.1573\n",
            "Epoch 429/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.0957 - acc: 0.3016 - val_loss: 2.8194 - val_acc: 0.1509\n",
            "Epoch 430/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.0793 - acc: 0.2903 - val_loss: 2.8194 - val_acc: 0.1595\n",
            "Epoch 431/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1278 - acc: 0.2846 - val_loss: 2.8069 - val_acc: 0.1530\n",
            "Epoch 432/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 2.1195 - acc: 0.2846 - val_loss: 2.7909 - val_acc: 0.1573\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1094 - acc: 0.2775 - val_loss: 2.8111 - val_acc: 0.1595\n",
            "Epoch 434/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.0937 - acc: 0.2846 - val_loss: 2.7970 - val_acc: 0.1616\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - 25s 270ms/step - loss: 2.1161 - acc: 0.2846 - val_loss: 2.8012 - val_acc: 0.1659\n",
            "Epoch 436/500\n",
            "89/89 [==============================] - 27s 290ms/step - loss: 2.0678 - acc: 0.3158 - val_loss: 2.8147 - val_acc: 0.1703\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - 24s 261ms/step - loss: 2.1001 - acc: 0.3016 - val_loss: 2.8287 - val_acc: 0.1573\n",
            "Epoch 438/500\n",
            "89/89 [==============================] - 22s 241ms/step - loss: 2.0852 - acc: 0.2860 - val_loss: 2.8052 - val_acc: 0.1552\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - 29s 320ms/step - loss: 2.1292 - acc: 0.2846 - val_loss: 2.7984 - val_acc: 0.1638\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - 29s 314ms/step - loss: 2.0952 - acc: 0.2853 - val_loss: 2.8016 - val_acc: 0.1638\n",
            "Epoch 441/500\n",
            "89/89 [==============================] - 28s 304ms/step - loss: 2.1130 - acc: 0.2853 - val_loss: 2.8091 - val_acc: 0.1638\n",
            "Epoch 442/500\n",
            "89/89 [==============================] - 29s 316ms/step - loss: 2.0846 - acc: 0.2938 - val_loss: 2.7952 - val_acc: 0.1573\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - 27s 296ms/step - loss: 2.0693 - acc: 0.3023 - val_loss: 2.8279 - val_acc: 0.1659\n",
            "Epoch 444/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.0841 - acc: 0.2732 - val_loss: 2.7867 - val_acc: 0.1681\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - 26s 290ms/step - loss: 2.0729 - acc: 0.3038 - val_loss: 2.8359 - val_acc: 0.1552\n",
            "Epoch 446/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.0826 - acc: 0.2917 - val_loss: 2.8117 - val_acc: 0.1638\n",
            "Epoch 447/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1282 - acc: 0.2796 - val_loss: 2.8391 - val_acc: 0.1638\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.0841 - acc: 0.3087 - val_loss: 2.7901 - val_acc: 0.1616\n",
            "Epoch 449/500\n",
            "89/89 [==============================] - 28s 302ms/step - loss: 2.1254 - acc: 0.2825 - val_loss: 2.8125 - val_acc: 0.1595\n",
            "Epoch 450/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.0770 - acc: 0.2881 - val_loss: 2.8100 - val_acc: 0.1573\n",
            "Epoch 451/500\n",
            "89/89 [==============================] - 29s 317ms/step - loss: 2.0701 - acc: 0.2796 - val_loss: 2.7957 - val_acc: 0.1595\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 2.1232 - acc: 0.2853 - val_loss: 2.7804 - val_acc: 0.1509\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - 31s 335ms/step - loss: 2.1145 - acc: 0.2881 - val_loss: 2.8192 - val_acc: 0.1595\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.0953 - acc: 0.2775 - val_loss: 2.8175 - val_acc: 0.1487\n",
            "Epoch 455/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.0763 - acc: 0.2988 - val_loss: 2.8156 - val_acc: 0.1616\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.0813 - acc: 0.2860 - val_loss: 2.8054 - val_acc: 0.1487\n",
            "Epoch 457/500\n",
            "89/89 [==============================] - 27s 302ms/step - loss: 2.1023 - acc: 0.2995 - val_loss: 2.7929 - val_acc: 0.1530\n",
            "Epoch 458/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.0765 - acc: 0.2910 - val_loss: 2.8320 - val_acc: 0.1552\n",
            "Epoch 459/500\n",
            "89/89 [==============================] - 28s 313ms/step - loss: 2.1432 - acc: 0.2732 - val_loss: 2.8227 - val_acc: 0.1681\n",
            "Epoch 460/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 2.1140 - acc: 0.2818 - val_loss: 2.8227 - val_acc: 0.1595\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - 29s 319ms/step - loss: 2.1050 - acc: 0.2945 - val_loss: 2.8274 - val_acc: 0.1595\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.0619 - acc: 0.3187 - val_loss: 2.8215 - val_acc: 0.1487\n",
            "Epoch 463/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.1100 - acc: 0.2903 - val_loss: 2.8103 - val_acc: 0.1466\n",
            "Epoch 464/500\n",
            "89/89 [==============================] - 22s 237ms/step - loss: 2.0704 - acc: 0.2945 - val_loss: 2.7955 - val_acc: 0.1509\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - 29s 310ms/step - loss: 2.1405 - acc: 0.2669 - val_loss: 2.8298 - val_acc: 0.1552\n",
            "Epoch 466/500\n",
            "89/89 [==============================] - 29s 311ms/step - loss: 2.1051 - acc: 0.2818 - val_loss: 2.7965 - val_acc: 0.1552\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - 24s 256ms/step - loss: 2.0972 - acc: 0.3045 - val_loss: 2.8270 - val_acc: 0.1530\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1182 - acc: 0.2896 - val_loss: 2.8038 - val_acc: 0.1573\n",
            "Epoch 469/500\n",
            "89/89 [==============================] - 28s 311ms/step - loss: 2.1338 - acc: 0.2860 - val_loss: 2.8299 - val_acc: 0.1638\n",
            "Epoch 470/500\n",
            "89/89 [==============================] - 24s 255ms/step - loss: 2.1081 - acc: 0.2924 - val_loss: 2.8063 - val_acc: 0.1552\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - 30s 327ms/step - loss: 2.0717 - acc: 0.2981 - val_loss: 2.7998 - val_acc: 0.1552\n",
            "Epoch 472/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.0818 - acc: 0.2775 - val_loss: 2.8077 - val_acc: 0.1552\n",
            "Epoch 473/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.0911 - acc: 0.2811 - val_loss: 2.8122 - val_acc: 0.1573\n",
            "Epoch 474/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1229 - acc: 0.2860 - val_loss: 2.7959 - val_acc: 0.1595\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - 29s 314ms/step - loss: 2.1033 - acc: 0.2796 - val_loss: 2.8313 - val_acc: 0.1552\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - 28s 305ms/step - loss: 2.0900 - acc: 0.3052 - val_loss: 2.8436 - val_acc: 0.1552\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - 27s 297ms/step - loss: 2.0666 - acc: 0.2811 - val_loss: 2.8497 - val_acc: 0.1509\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - 27s 293ms/step - loss: 2.1043 - acc: 0.2740 - val_loss: 2.8180 - val_acc: 0.1530\n",
            "Epoch 479/500\n",
            "89/89 [==============================] - 28s 309ms/step - loss: 2.0770 - acc: 0.2938 - val_loss: 2.8080 - val_acc: 0.1530\n",
            "Epoch 480/500\n",
            "89/89 [==============================] - 31s 341ms/step - loss: 2.1254 - acc: 0.2789 - val_loss: 2.8093 - val_acc: 0.1509\n",
            "Epoch 481/500\n",
            "89/89 [==============================] - 27s 292ms/step - loss: 2.0870 - acc: 0.2910 - val_loss: 2.7960 - val_acc: 0.1573\n",
            "Epoch 482/500\n",
            "89/89 [==============================] - 26s 289ms/step - loss: 2.0741 - acc: 0.3059 - val_loss: 2.7961 - val_acc: 0.1595\n",
            "Epoch 483/500\n",
            "89/89 [==============================] - 28s 308ms/step - loss: 2.0560 - acc: 0.2924 - val_loss: 2.8301 - val_acc: 0.1487\n",
            "Epoch 484/500\n",
            "89/89 [==============================] - 27s 301ms/step - loss: 2.1145 - acc: 0.2839 - val_loss: 2.8062 - val_acc: 0.1509\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - 27s 300ms/step - loss: 2.0855 - acc: 0.2931 - val_loss: 2.8027 - val_acc: 0.1573\n",
            "Epoch 486/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1270 - acc: 0.2811 - val_loss: 2.8126 - val_acc: 0.1552\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - 27s 294ms/step - loss: 2.1280 - acc: 0.2683 - val_loss: 2.8237 - val_acc: 0.1509\n",
            "Epoch 488/500\n",
            "89/89 [==============================] - 22s 242ms/step - loss: 2.0722 - acc: 0.2889 - val_loss: 2.8251 - val_acc: 0.1509\n",
            "Epoch 489/500\n",
            "89/89 [==============================] - 28s 303ms/step - loss: 2.1088 - acc: 0.2818 - val_loss: 2.8346 - val_acc: 0.1552\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - 31s 330ms/step - loss: 2.0979 - acc: 0.2952 - val_loss: 2.8494 - val_acc: 0.1509\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - 28s 301ms/step - loss: 2.1285 - acc: 0.2903 - val_loss: 2.8396 - val_acc: 0.1509\n",
            "Epoch 492/500\n",
            "89/89 [==============================] - 23s 245ms/step - loss: 2.0650 - acc: 0.3059 - val_loss: 2.8065 - val_acc: 0.1573\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 2.0957 - acc: 0.2690 - val_loss: 2.8324 - val_acc: 0.1487\n",
            "Epoch 494/500\n",
            "89/89 [==============================] - 28s 312ms/step - loss: 2.1044 - acc: 0.2775 - val_loss: 2.8338 - val_acc: 0.1509\n",
            "Epoch 495/500\n",
            "89/89 [==============================] - 28s 310ms/step - loss: 2.1650 - acc: 0.2704 - val_loss: 2.8288 - val_acc: 0.1552\n",
            "Epoch 496/500\n",
            "89/89 [==============================] - 27s 298ms/step - loss: 2.0944 - acc: 0.2811 - val_loss: 2.8161 - val_acc: 0.1552\n",
            "Epoch 497/500\n",
            "89/89 [==============================] - 29s 315ms/step - loss: 2.0852 - acc: 0.2903 - val_loss: 2.8167 - val_acc: 0.1595\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - 31s 335ms/step - loss: 2.0783 - acc: 0.2988 - val_loss: 2.7951 - val_acc: 0.1573\n",
            "Epoch 499/500\n",
            "89/89 [==============================] - 27s 291ms/step - loss: 2.1259 - acc: 0.2931 - val_loss: 2.7795 - val_acc: 0.1638\n",
            "Epoch 500/500\n",
            "89/89 [==============================] - 27s 295ms/step - loss: 2.1006 - acc: 0.3006 - val_loss: 2.7933 - val_acc: 0.1466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kwylTJpTP5XI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "308a7ff7-9216-43af-94c3-aec50bc31737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the model"
      ],
      "metadata": {
        "id": "lD-vKaoHQAFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/My Drive/cut_panoramic/Model/Classification', exist_ok=True)\n",
        "model.save('/content/drive/My Drive/cut_panoramic/Model/Classification/Male/M4_New_Train_500_Unfreeze.h5')"
      ],
      "metadata": {
        "id": "74dL7-HLP_Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('/content/drive/My Drive/cut_panoramic/Model/33_รอบที่3_Flimpano_Male125_250.h5')"
      ],
      "metadata": {
        "id": "qcPW-brHQDpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4pe9URV1vBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}